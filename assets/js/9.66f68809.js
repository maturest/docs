(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{285:function(t,_,v){"use strict";v.r(_);var a=v(14),r=Object(a.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h1",{attrs:{id:"百度千帆-私有模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#百度千帆-私有模型"}},[t._v("#")]),t._v(" 百度千帆-私有模型")]),t._v(" "),_("h2",{attrs:{id:"模型训练的生命周期"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#模型训练的生命周期"}},[t._v("#")]),t._v(" 模型训练的生命周期：")]),t._v(" "),_("p",[t._v("数据导入 ===> 数据标注 ===> 训练配置 ===> 模型纳管 ===> 发布服务 ===> 体验测试")]),t._v(" "),_("h2",{attrs:{id:"数据导入"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据导入"}},[t._v("#")]),t._v(" 数据导入")]),t._v(" "),_("p",[t._v("标注类型：")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("文本对话")])]),t._v(" "),_("li",[_("p",[t._v("泛文本无标注（特定行业方向/场景下的大规模无标注数据语料，作为后续post-pretrain的训练输入）")])]),t._v(" "),_("li",[_("p",[t._v("query 问题集（单轮或多轮的提问语料数据，作为后续RLHF的训练输入）")])])]),t._v(" "),_("p",[t._v("标注模版：")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("排序（单轮或多轮的文本对话数据，单个提问对应多个回答，需要对多个回答进行排序，作为后续奖励模型的训练输入。）")])]),t._v(" "),_("li",[_("p",[t._v("非排序（单轮或多轮的文本对话数据，单个提问和回答一一对应，作为后续SFT、prompt tuning以及delta tuning的训练输入。）")])])]),t._v(" "),_("h2",{attrs:{id:"模型精调"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#模型精调"}},[t._v("#")]),t._v(" 模型精调")]),t._v(" "),_("blockquote",[_("p",[_("strong",[t._v("1、希望能在大模型中注入领域知识，增强模型领域专业性，可推荐使用Post-pretrain。\n2、若仅有少量高质量语料或费用敏感用户，建议考虑知识库管理方法，学习领域知识。")])])]),t._v(" "),_("p",[_("img",{attrs:{src:"https://bce.bdstatic.com/doc/bml_enterprise/wenxinqianfanshare/image_e4868fa.png",alt:"Post-pretrain"}})]),t._v(" "),_("h3",{attrs:{id:"post-pretrain-预训练"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#post-pretrain-预训练"}},[t._v("#")]),t._v(" Post-pretrain（预训练）")]),t._v(" "),_("p",[t._v("什么时候我们考虑使用Post-pretrain？有以下几点：")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("有大量高质量(Billion级tokens)的行业预训练语料；")])]),t._v(" "),_("li",[_("p",[t._v("有少量指令集(千条数据)，或有自主构建指令集的能力；")])]),t._v(" "),_("li",[_("p",[t._v("了解post-pretrain全流程（垂直领域Post-pretrain、通用SFT、垂直领域SFT），且接受平台计费。")])])]),t._v(" "),_("h4",{attrs:{id:"数据集准备"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据集准备"}},[t._v("#")]),t._v(" 数据集准备")]),t._v(" "),_("p",[t._v("要在特定领域使用post-pretrain方法训练一个大模型，关键是准备与该领域相关的高质量语料。")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("数据质量（高，我们可以提供尽可能多的物料，然后再进行数据清洗、过滤）")])]),t._v(" "),_("li",[_("p",[t._v("数据规模（多，10亿的 tokens, 约 13 亿的汉字，但尝试申请预训练的订单，提示最低要 1千万 tokens,那么也就最低大概需要"),_("span",{staticStyle:{color:"red"}},[t._v("一千三百万个汉字")]),t._v("）")])])]),t._v(" "),_("p",[_("span",{staticStyle:{color:"red"}},[t._v("难题1：由于我们的目的是想要训练私有化模型，并且具有李氏易学的特色，显然预训练模式是最佳实践方式，那么泛文本无标注数据集哪里来？")])]),t._v(" "),_("p",[_("span",{staticStyle:{color:"blue"}},[t._v("答：生命智者直播物料，蓝狮子文字稿，淙翰心语，荔枝文字稿，优听广播。以上内容是找李钰迪师姐索要，其次有关的文案是从公众号推文入手（考虑到内容的不是李老师的风格，就先做备选）。")]),_("span",{staticStyle:{color:"red"}},[t._v("但目前遇到的卡点是数据集太少，要想预训练的话，数据集至少要1千万tokens,换句话也就是要1千百万个汉字，跟侯总沟通之后，算下来所有人去找对应的数据集筹够是不可能的，另外的一个思路是从网上下载很多篇小说，然后尝试训练。。。其次就是按官方推荐的尝试构建知识库。")])]),t._v(" "),_("h4",{attrs:{id:"数据处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据处理"}},[t._v("#")]),t._v(" 数据处理")]),t._v(" "),_("p",[_("img",{attrs:{src:"https://pic.imgdb.cn/item/65780611c458853aef47e769.jpg",alt:""}})]),t._v(" "),_("p",[t._v("1、异常清洗：文档内修改/去除问题字符串，比如移除不可见字符，规范化空格，去除乱码，去除网页标识符，去除表情等。")]),t._v(" "),_("p",[t._v("2、过滤：根据一些规则或模型指标，从语料库中删除整个文档，比如检查文档的词数目,检查文档的字重复率,检查文档的词重复率,检查文档的特殊字符率,检查文档的色情暴力词率,检查文档的语言概率,检查文档的困惑度。")]),t._v(" "),_("p",[t._v("3、去重：去除相似文档，减少算力浪费，降低过拟合风险。")]),t._v(" "),_("p",[t._v("4、去隐私：匿名化，去除账号、密码、电话号码。")]),t._v(" "),_("h4",{attrs:{id:"数据配置"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据配置"}},[t._v("#")]),t._v(" 数据配置")]),t._v(" "),_("ul",[_("li",[t._v("Epoch大于1有风险么？")])]),t._v(" "),_("blockquote",[_("p",[t._v("Epoch等价于数据重复的次数。目前的实验表明，post-pretrain阶段重复的数据对模型训练没有额外增益，甚至是有害的，而且复制的数据会耗费额外的计算资源，导致训练速度变慢，故平台默认值为1。")])]),t._v(" "),_("ul",[_("li",[t._v("学习率需要如何调整？")])]),t._v(" "),_("blockquote",[_("p",[t._v("学习率（LearningRate）是在梯度下降的过程中更新权重时的超参数，学习率过高会导致模型难以收敛，并且会加快遗忘，同时增加训练的不稳定性。学习率过低则会导致模型收敛速度过慢，平台已给出默认推荐值，如果没有专业的调优经验，推荐使用默认值。")])]),t._v(" "),_("p",[t._v("关于混合训练：仅用单一领域数据进行模型训练，模型很容易出现灾难性遗忘现象，其他领域的能力出现下降。在领域训练过程中加入通用数据进行混合训练，在增强用户垂类场景能力的同时，保持其原本的通用能力。")]),t._v(" "),_("p",[_("strong",[t._v("使用建议：")])]),t._v(" "),_("ul",[_("li",[t._v("若用户仅需要使用指定垂类场景下的能力，可以直接进行训练。")]),t._v(" "),_("li",[t._v("若用户需要模型保持通用能力的同时，提升垂类场景的能力，可以选择数据配比进行混合训练。\n默认配比为1:5，即1份领域数据: 5份通用语料。")])]),t._v(" "),_("h3",{attrs:{id:"sft-有监督微调"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#sft-有监督微调"}},[t._v("#")]),t._v(" SFT（有监督微调）")]),t._v(" "),_("p",[t._v("有监督微调（SFT）是指采用预先训练好的神经网络模型，并针对你自己的专门任务在少量的监督数据上对其进行重新训练的技术。")]),t._v(" "),_("p",[t._v("1、确保已准备好大模型的训练数据，或前往数据集管理中标注数据")]),t._v(" "),_("p",[t._v("2、利用准备好的训练数据及平台提供的预训练大模型，训练自己的大模型。")]),t._v(" "),_("p",[t._v("3、完成SFT后，可以发布训练后的模型，或继续在RLHF训练过程对模型继续训练。")]),t._v(" "),_("h3",{attrs:{id:"奖励模型训练"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#奖励模型训练"}},[t._v("#")]),t._v(" 奖励模型训练")]),t._v(" "),_("p",[t._v("1、确保已准备好大模型的训练数据，或前往数据集管理中标注数据")]),t._v(" "),_("p",[t._v("2、利用准备好的训练数据及平台提供的预训练大模型，训练自己的大模型。")]),t._v(" "),_("p",[t._v("3、完成SFT后，可以发布训练后的模型，或继续在RLHF训练过程对模型继续训练。")]),t._v(" "),_("h3",{attrs:{id:"强化学习训练"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#强化学习训练"}},[t._v("#")]),t._v(" 强化学习训练")]),t._v(" "),_("p",[t._v("1、确保已准备好奖励模型的训练数据，或前往数据集管理标注数据。")]),t._v(" "),_("p",[t._v("2、使用准备好的训练数据及平台提供的预训练奖励模型，训练自己的奖励模型。")]),t._v(" "),_("p",[t._v("3、确保已准备好强化学习的训练数据，或前往数据集管理标注数据。")]),t._v(" "),_("p",[t._v("4、使用训练数据及强化学习机制，进一步训练大模型，确保模型效果贴近业务场景。")]),t._v(" "),_("p",[t._v("5、完成强化学习训练后，可以发布训练后的模型到模型仓库。")]),t._v(" "),_("h3",{attrs:{id:"关于收费"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#关于收费"}},[t._v("#")]),t._v(" 关于收费")]),t._v(" "),_("ul",[_("li",[_("p",[t._v("数据集模块的功能免费")])]),t._v(" "),_("li",[_("p",[t._v("模型微调，预训练模块（最低需要1千万tokens,并且收费）")])]),t._v(" "),_("li",[_("p",[t._v("模评估需要收费")])]),t._v(" "),_("li",[_("p",[t._v("模型发布之后（当前私有资源池按照租赁方式计费的算力单元单价如下：按小时的租赁方式：RMB￥20 元/小时；按天数的租赁方式：RMB￥250 元/天。）")])])]),t._v(" "),_("h2",{attrs:{id:"总结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),_("p",[_("strong",[t._v("从数据集的数量和私有模型租赁费用两个维度去考虑的话，还是推荐知识库。")])]),t._v(" "),_("p",[t._v("1、因为我们本身的数据集就很少（至少需要一千三百万的字数），其次模型发布之后的费用较高（一月7.5k，一年9万）。")]),t._v(" "),_("p",[t._v("2、另外就算我们花大量的精力和时间去凑够数据集，也不考虑费用问题的画，训练出来的大模型也未必如期所愿，存在一个未知数。")]),t._v(" "),_("p",[t._v("3、官方也推荐若仅有少量高质量语料或费用敏感用户，建议考虑知识库管理方法，学习领域知识。")])])}),[],!1,null,null,null);_.default=r.exports}}]);